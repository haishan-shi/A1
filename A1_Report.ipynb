{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1_Report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haishan-shi/A1/blob/master/A1_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XqYqGHz8o8c",
        "colab_type": "text"
      },
      "source": [
        "# The Link of the report\n",
        "\n",
        "https://github.com/haishan-shi/A1/blob/master/A1_Report.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ZnEZ4R5Bid",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Gradient_Based Learning Applied to Document Recognition\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_nP3U45Bif",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwHzqxap5Bih",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq_73d825Bii",
        "colab_type": "text"
      },
      "source": [
        "As Yann’s team discussed in the report, it is shown that multilayer neural network trained by backward programming algorithm is the best example of a successful gradient-based learning technique. The gradient-based learning algorithm involves computing the gradient of some form of performance measurement in the weight space, whether accurate or approximate, and then using the result to determine the weight change in some appropriate way (Williams & Zipser 1995). Also, they indicated several approaches of handwritten character recognition and compared them in the processing of standard handwritten numeral recognition tasks. Back-propagation learning has been successfully applied to a large, real-life task. A preliminary study of alphanumeric characters shows that this method can be directly extended to larger tasks. The connection network and weights obtained by back propagation learning can be easily implemented on commercial digital signal processing hardware. Throughput from camera to classified image exceeds 10 digits per second (LeCun et al. 1990). “Convolutional neural network provides partial invariance to translation, rotation, scale and deformation” (Lawrence et al. 1997).\n",
        "\n",
        "Plus, they introduced that Graph Transformer Network (GTN), which applies the gradient-based learning algorithm to a modular system with graph as input and graph as output. The training is to compute the gradient of the global objective function relative to all parameters in the system by accessing a back propagation program. The report also introduced a complete cheque reading system based on these concepts, which employs the character recognizer of CNN, integrating with global training technology, to supply record accuracy in organisations and individual inspection (Bottou, Bengio & Le Cun 1997).\n",
        "\n",
        "In addition, two main challenges, in the report, are attempted to resolve:\n",
        "1.\tIn the research of online Handwriting Recognition Systems, Characters taken in isolation will be too ambiguous when interaction with pen-based devices is highly to be improved. \n",
        "2.\tExperimenting learning in Real Handwriting Recognition Systems, the recognition of individual characters and the separation of characters from adjacent characters in words or sentences are very difficult. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyUl3k7Y5Bij",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l15bWXqh5Bik",
        "colab_type": "text"
      },
      "source": [
        "First of all, the creative idea in the report is performing a popular minimization procedure - the stochastic gradient algorithm. The computational complexity of learning algorithms becomes a key constraint in predicting large data sets. This contribution refines a stochastic gradient algorithm for large-scale machine learning issues. The report described the stochastic gradient algorithm, the reason why the stochastic gradient algorithm is attractive when the data is abundant, and discussed the asymptotic efficiency of the estimation after the training set is passed once and empirical evidence (Bottou 2010).\n",
        "\n",
        "Furthermore, another creative idea in the report is using Viterbi algorithm (VA), which is a decoding method of convolutional codes proposed in 1967. Since then, it has been considered an attractive solution to various digital estimation problems, a bit like Kalman filter has adapted to various analog estimation problems. Like Kalman filter, VA uses a recursive method to track the state of stochastic process. This method is optimal in a sense and easy to implement and analyse (Forney 1973).\n",
        "\n",
        "Also, the report provided an example of a Graph Transformer Module, and it was clearly shown that this concept could be used to the environment where knowledge or state information in many areas could be graphically represented. To more specifically analyse the GTN, they indicated an overly simple example of a trainable system constructed by three graph transformers in the context of handwriting recognition. The mission of the system aims to find the most applicable handwriting segmentation into characters. In addition, performing the concept of GTN to establish a checking amount reading system. The system found candidate areas, selected the most appropriate number of fields, divided these segments into candidate characters, read and evaluated the candidate characters, and eventually applied context knowledge (such as stochastic grammar for check amounts) to find the best explanation of them (Bottou, Bengio & Le Cun 1997).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zN7TZ455Bil",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuiNCD75Bim",
        "colab_type": "text"
      },
      "source": [
        "The technical development in the report is high quality (giving the score 8 out of 10). There are several reasons shown as below: \n",
        "1.\tThe report comparing classifiers based on their accuracy on a training set\n",
        "would be of high quality as the accuracy performed on a training set is lower than the actual test error. For example, as the article described: “when comparing to training based only on the errors performed at the character level, with a 25461-word dictionary, errors dropped from 4.6% and 2.0% word and character errors to 3.2% and 1.4% respectively after word-level training”. \t \n",
        "2.\tThe results in the report were described clearly and easily understood. For instance, the benefits of global training and the flexibility of GTNs were precisely demonstrated through performing experiments.\n",
        "3.\tConclusions were too precisely through presenting several points and solutions so that it is difficult to capture the main points in the conclusion part shortly. For example, five general solutions for the existing problems in the report encountered in recognition system were provided. In my opinion, this part can be removed or simply presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OvrO2Is5Bin",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKC9gqq5Bio",
        "colab_type": "text"
      },
      "source": [
        "Graph Transformer Network relies on the applicability of gradient-based learning algorithm to a system consisting of modules with graph as input and graph as output. The training is to computing the gradient of the global targeted function related to all parameters in the system by accessing a back propagation program. Gradient-based learning algorithm presents a detailed cheque reading system relied on these ideas. The system can apply CNN character recogniser, incorporating with global training technology, to possess record accuracy at work and individual check (Bengio, Bottou & LeCun 2000).\n",
        "\n",
        "For further development, gradient-based learning algorithms to address these problems can analyse samples to predict the gradient of the loss function. It is recommended to discuss formalism of directed acyclic graphs of stochastic computational graphs, which includes definitive functions and conditional possibility distributions, and present how to automatically obtain unbiased estimates of loss function gradients. The algorithm for computing gradient estimator is a simple modification of the standard back propagation algorithm. For example, estimators obtained from various prior work and variance reduction techniques are proposed. It can help researchers develop complex models, including combinations of random and deterministic operations (Schulman et al. 2015).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj9kbgsR5Bip",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwNoGdSe5Biq",
        "colab_type": "text"
      },
      "source": [
        "The overall structure is very clear. It could be understood the background of the challenges, problems and the purpose of the report. I would rate 8 out of 10. There are several reasons described as below:\n",
        "1.\tSample\n",
        "\n",
        "  In order to provide credible conclusions, the sample in the report is representative and sufficient. Representativeness is the method of selection and distribution in the article. For example, graphs and data set are used in the computation of formulas and the appropriate sample size was determined by data analysis.\n",
        "2.\tData analysis\n",
        "\n",
        "  The report provided appropriate statistical tests based on the data set, and satisfied the assumptions. In many comparisons, graphs and distribution of data are clearly labeled and indicated the result and impact of variables.\n",
        "3.\tConclusion\n",
        "\n",
        "  The conclusions of the report are consistent with the results and the limitations of the research are presented and mentioned. The results are in line with the expectations of the researchers, and the results that do not meet the expectations of the researchers and future work are also concerned.\n",
        "4.\tRelevance\n",
        "\n",
        "  The materials in the report are more detailed and coherent, and the relevance of the content is clearly stated.\n",
        "5.\tPresentation style \n",
        "\n",
        "  The presentation style addressed purpose and problems of the research in the report, and also analysed the random distribution experiment design and pre-experiment design based on the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM7hfIBM5Bis",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Bengio, Y., Bottou, L. & LeCun, Y.A. 2000, 'Module for constructing trainable   modular network in which each module inputs and outputs data structured as a graph', Series Module for constructing trainable modular network in which each module inputs and outputs data structured as a graph Google Patents.\n",
        "\n",
        "Bottou, L. 2010, 'Large-scale machine learning with stochastic gradient descent', Proceedings of COMPSTAT'2010, Springer, pp. 177-86.\n",
        "\n",
        "Bottou, L., Bengio, Y. & Le Cun, Y. 1997, 'Global training of document processing systems using graph transformer networks', IEEE, pp. 489-94.\n",
        "\n",
        "Forney, G.D. 1973, 'The viterbi algorithm', Proceedings of the IEEE, vol. 61, no. 3, pp. 268-78.\n",
        "\n",
        "Lawrence, S., Giles, C.L., Tsoi, A.C. & Back, A.D. 1997, 'Face recognition: A convolutional neural-network approach', IEEE transactions on neural networks, vol. 8, no. 1, pp. 98-113.\n",
        "\n",
        "LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E. & Jackel, L.D. 1990, 'Handwritten digit recognition with a back-propagation network', pp. 396-404.\n",
        "\n",
        "Schulman, J., Heess, N., Weber, T. & Abbeel, P. 2015, 'Gradient estimation using stochastic computation graphs', pp. 3528-36.\n",
        "\n",
        "Williams, R.J. & Zipser, D. 1995, 'Gradient-based learning algorithms for recurrent', Backpropagation: Theory, architectures, and applications, vol. 433.\n"
      ]
    }
  ]
}